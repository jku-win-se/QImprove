{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66437528-1d8d-499b-9a86-6af6ce7ae028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ab4a8-1d91-49e2-8571-4258bfa83a13",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\">\n",
    "    WARNING:</span> This file is meant to be executed top to bottom. \n",
    "</h1><h2>There are quite a few parts copy & pasted. Hence, MANY variables are shadowed.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f43b0e-5548-48ec-b708-952f15d1ef48",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 0. Data Production: Each of these cells takes 14 minutes to execute on Stefan's machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee81697-6b1e-48eb-affc-187f58796afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREFIX = \"last\"\n",
    "THRESHOLD = 0.000001  # 1.0 and 0.9999999... are the same, right?\n",
    "\n",
    "raw_results_path = Path(\"/Volumes/Transcend/data/QRefactoring-results-final/QRefactoring-results-final\")\n",
    "\n",
    "OL_file_path = Path(f\"{DATA_PREFIX.replace('%', '')}_vals_OL.pkl\")\n",
    "OED_file_path = Path(f\"{DATA_PREFIX.replace('%', '')}_vals_OED.pkl\")\n",
    "\n",
    "paper_tables_dir = Path(\"/Users/stefan/Library/CloudStorage/Dropbox/Apps/Overleaf/ASE2023 - QRepair/generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85106ce-c5e1-4e16-9af9-91d45da38688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper!\n",
    "def extract_best_solutions(files, earliest_finish=None):\n",
    "    if earliest_finish == \"last\":  # override last\n",
    "        earliest_finish = None\n",
    "    \n",
    "    earliest_finish_times = {}\n",
    "    if earliest_finish:\n",
    "        for earliest_finish_file in list(raw_results_path.glob(\"*earliest_finish.csv\")):\n",
    "            problem = earliest_finish_file.stem.replace(\"_earliest_finish\", \"\")\n",
    "\n",
    "            earliest_finish_df = pd.read_csv(earliest_finish_file)\n",
    "            earliest_finish_time = earliest_finish_df.iloc[0][earliest_finish]\n",
    "            earliest_finish_times[problem] = earliest_finish_time\n",
    "    # print(\"Earliest finish times:\")\n",
    "    # pprint(earliest_finish_times)\n",
    "    best_OL_rows = []\n",
    "    best_OED_rows = []\n",
    "    # extract last gen value\n",
    "    for pi_file in sorted(files):\n",
    "        problem, seed, option = extract_info_from_file(pi_file)\n",
    "        qubits, arbitrary = QUBITS_and_ARBITRARY[problem]\n",
    "        \n",
    "        last_row = dict(problem=problem, option=option, seed=seed, qubits=qubits, arbitrary=arbitrary)\n",
    "        results_file_df = pd.read_csv(pi_file)\n",
    "        if earliest_finish:\n",
    "            if problem not in earliest_finish_times:\n",
    "                print(\"WARNING!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                print(\"No earliest finish time for problem\", problem)\n",
    "            else:\n",
    "                results_file_df = results_file_df[results_file_df.timestamp <= earliest_finish_df[earliest_finish].iloc[0]]\n",
    "        \n",
    "        last_gen_df = results_file_df[results_file_df.ngen == results_file_df.ngen.max()].reset_index()\n",
    "        last_gen_df[\"no_error_prob_actual\"] = 1 - last_gen_df.apply(lambda row: get_actual_error_rate(row.num_gates, row.num_nonloc_gates), axis=1)\n",
    "        last_gen_df[\"OED\"] = last_gen_df.overlap * last_gen_df.no_error_prob_actual\n",
    "        \n",
    "        best_OL_row = dict(last_row)\n",
    "        best_OL_row.update(last_gen_df.sort_values(\"overlap\", ascending=False).iloc[0].to_dict())\n",
    "        best_OL_rows.append(best_OL_row)\n",
    "        \n",
    "        best_OED_row = dict(last_row)\n",
    "        best_OED_row.update(last_gen_df.sort_values(\"OED\", ascending=False).iloc[0].to_dict())\n",
    "        best_OED_rows.append(best_OED_row)\n",
    "\n",
    "    OL_df = pd.DataFrame(best_OL_rows).sort_values(by=[\"problem\", \"option\", \"seed\"])\n",
    "    OED_df = pd.DataFrame(best_OED_rows).sort_values(by=[\"problem\", \"option\", \"seed\"])\n",
    "    \n",
    "    return OL_df, OED_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37351c57-c52e-40e5-b122-cf78b6e95b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if OL_file_path.exists() and OED_file_path.exists():\n",
    "    print(\"Found data files. No need to extract.\")\n",
    "else:\n",
    "    print(\"Data files not found. Starting the data extraction.\")\n",
    "    \n",
    "    output_files = list(raw_results_path.glob(\"*.csv\"))\n",
    "    output_files = [f for f in output_files if \"logbook\" not in str(f)]\n",
    "    output_files = [f for f in output_files if \"seed\" in str(f)]\n",
    "    output_files = [f for f in output_files if \"PI.csv\" not in str(f)]\n",
    "    output_files = [f for f in output_files if \"DCI.csv\" not in str(f)]\n",
    "    output_files = [f for f in output_files if \"HVrefpoint.csv\" not in str(f)]\n",
    "    output_files = [f for f in output_files if \"globalPareto.csv\" not in str(f)]\n",
    "    output_files = [f for f in output_files if \"earliest_finish.csv\" not in str(f)]\n",
    "          \n",
    "          \n",
    "    OL_df, OED_df = extract_best_solutions(output_files, earliest_finish=DATA_PREFIX)\n",
    "    OL_df.to_pickle(OL_file_path)\n",
    "    OED_df.to_pickle(OED_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d9507-d002-41b6-8cbb-465399eed988",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RQ 1.1 & 2.1 (Theoretical gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc0967-2768-4867-bd8d-629607b64072",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.read_pickle(OL_file_path)\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df60beb-3403-43c1-ad27-c855b41c5eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _check_improvement(row): \n",
    "    ref_num_gates, ref_depth, ref_non_local = reference_fitness_values[row.problem]\n",
    "    # either all equal, or some better, some worse...\n",
    "    if (row.num_gates == ref_num_gates and row.depth == ref_depth and row.num_nonloc_gates == ref_non_local) or \\\n",
    "       ((row.num_gates > ref_num_gates or row.depth > ref_depth or row.num_nonloc_gates > ref_non_local) and \\\n",
    "        (row.num_gates < ref_num_gates or row.depth < ref_depth or row.num_nonloc_gates < ref_non_local)):\n",
    "        return \"Pareto Equal\"\n",
    "    \n",
    "    # not all equal, not pareto_equal, so it's either better or worse\n",
    "    if row.num_gates >= ref_num_gates and row.depth >= ref_depth and row.num_nonloc_gates >= ref_non_local:\n",
    "        return \"Worse\"\n",
    "    \n",
    "    # one of them is better\n",
    "    if row.num_gates <= ref_num_gates and row.depth <= ref_depth and row.num_nonloc_gates <= ref_non_local:\n",
    "        return \"Optimized\"\n",
    "\n",
    "def get_operator_categorisation_OL(row):   \n",
    "    if (1.0 - row.overlap) > THRESHOLD:  # TODO: Check if this is the overlap? ...\n",
    "        return \"Faulty\"\n",
    "    \n",
    "    # it's not buggy, so check if we are better or not...\n",
    "    return _check_improvement(row) \n",
    "    \n",
    "def get_operator_categorisation_OED(row):\n",
    "    ref_num_gates, ref_depth, ref_non_local = reference_fitness_values[row.problem]\n",
    "    ref_OED_actual = 1 - get_actual_error_rate(ref_num_gates, ref_non_local)\n",
    "    \n",
    "    if (ref_OED_actual - row.OED) > THRESHOLD:\n",
    "        return \"Faulty\"\n",
    "    \n",
    "    # it's not buggy, so check if we are better or not...\n",
    "    return _check_improvement(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2c08f-608e-40f5-b257-9974d2b95667",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ 1.1 - Repair (theoretical)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3de4c-88b4-4374-9908-e0a6acc7eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_df = all_vals_df[all_vals_df.repair].reset_index()\n",
    "repair_df[\"Categorisation_OL\"] = repair_df.apply(get_operator_categorisation_OL, axis=1)\n",
    "# repair_df[\"Categorisation_OED\"] = repair_df.apply(get_operator_categorisation_OED, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd1433-efc3-4ab9-8f9f-48195af4a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(repair_df.groupby(\"Categorisation_OL\").count())\n",
    "# display(repair_df.groupby(\"Categorisation_OED\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2547e6-54f8-4825-acf2-2fba767b86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for opt in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[opt])\n",
    "    rows = []\n",
    "    for problem in repair_df.problem.unique():\n",
    "        qubits=repair_df[repair_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = repair_df[repair_df.problem == problem].arbitrary.iloc[0]\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=\"Arbitrary\" if arbitrary else \"Specific\" )\n",
    "        \n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(repair_df[\n",
    "                (repair_df.option == opt) & \n",
    "                (repair_df.problem == problem) & \n",
    "                (repair_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab.problem)\n",
    "    tables[opt] = tab\n",
    "    # display(tab)\n",
    "\n",
    "# sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "# sum_table[\"problem\"] = sum_table.problem.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "# sum_table[\"Qubits\"] = sum_table.qubits.apply(lambda v: v.split(\"/\")[0].strip() + \" qubits\")\n",
    "# sum_table[\"Arbitrary\"] = sum_table.Arbitrary.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "\n",
    "# sum_table[\"Problem\"] = \"[\" + sum_table[\"Arbitrary\"] + \"]\" + sum_table[\"problem\"]\n",
    "# sum_table = sum_table.sort_values(by=[\"Qubits\", \"problem\"], key=lambda col: col.str.lower())\n",
    "# sum_table.index = pd.MultiIndex.from_frame(sum_table[[\"Qubits\", \"Problem\"]])\n",
    "# sum_table = sum_table.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"Problem\", \"Qubits\"])\n",
    "\n",
    "\n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "sum_table[\"problem\"] = sum_table.problem.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "sum_table[\"Qubits\"] = sum_table.qubits.apply(lambda v: v.split(\"/\")[0].strip() + \" qubits\")\n",
    "sum_table[\"Input State\"] = sum_table.Arbitrary.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "\n",
    "sum_table[\"Problem\"] =  sum_table[\"problem\"] + \" (\" + sum_table[\"Qubits\"]+\")\"\n",
    "sum_table = sum_table.sort_values(by=[\"Arbitrary\", \"qubits\"], ascending=[False,True], key=lambda col: col.str.lower())\n",
    "sum_table.index = pd.MultiIndex.from_frame(sum_table[[\"Input State\", \"Problem\"]])\n",
    "sum_table = sum_table.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"Problem\", \"Qubits\", \"Input State\"])\n",
    "\n",
    "# RQ11_table = sum_table.set_index(\"Problem\").sort_values(by=[\"qubits\", \"problem\"], key=lambda col: col.str.lower()).drop(columns=[\"problem\", \"Qubits\", \"Arbitrary\"])\n",
    "RQ11_table = sum_table\n",
    "RQ11_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3ffbe-5c85-41f2-943f-5ed413a978fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate Relative Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1584d-1714-4397-b5ca-b1a134d33b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interesting_cols = [\"num_gates\", \"depth\", \"num_nonloc_gates\", \"num_parameters\"]\n",
    "repair_df[repair_df.Categorisation_OL == \"Optimized\"][interesting_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51113ad0-4b01-459b-8e65-6b9906fef2a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ2.1 - Optimization (theoretical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504718e-443d-450b-bc48-58a74becd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df[all_vals_df.repair == False].reset_index()\n",
    "optimization_df[\"Categorisation_OL\"] = optimization_df.apply(get_operator_categorisation_OL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047e16f-21c4-4ac5-b771-fa3916f55b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arbitrary)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "\n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "sum_table[\"problem\"] = sum_table.problem.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "sum_table[\"Qubits\"] = sum_table.qubits.apply(lambda v: v.split(\"/\")[0].strip() + \" qubits\")\n",
    "sum_table[\"Arbitrary\"] = sum_table.Arbitrary.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "\n",
    "sum_table[\"Problem\"] = \"[\" + sum_table[\"Arbitrary\"] + \"]\" + sum_table[\"problem\"]\n",
    "sum_table = sum_table.sort_values(by=[\"Qubits\", \"problem\"], key=lambda col: col.str.lower())\n",
    "sum_table.index = pd.MultiIndex.from_frame(sum_table[[\"Qubits\", \"Problem\"]])\n",
    "sum_table = sum_table.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"Problem\", \"Qubits\"])\n",
    "\n",
    "# RQ11_table = sum_table.set_index(\"Problem\").sort_values(by=[\"qubits\", \"problem\"], key=lambda col: col.str.lower()).drop(columns=[\"problem\", \"Qubits\", \"Arbitrary\"])\n",
    "RQ21_table = sum_table\n",
    "RQ21_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3a76a-7d33-4156-8abd-0570bb355a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Summary Tables (because the full table is too long!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a48d1-e3da-45cb-8a8e-4f3ea0da64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    \n",
    "    \n",
    "    tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    tables[option] = pd.DataFrame(tab)\n",
    "        \n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "\n",
    "RQ21_summary_table_sum = sum_table.T.drop(columns=[\"qubits\", \"problem\", \"Arbitrary\"])\n",
    "RQ21_summary_table_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc73cc-99ae-4197-80dc-15e6894ceb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    tab = tab.groupby(\"Arbitrary\").sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "\n",
    "RQ21_summary_table_input_state = sum_table\n",
    "RQ21_summary_table_input_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5241a-b919-4a2a-bb7c-9838a34777b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    \n",
    "    tab = tab.groupby(\"qubits\").sum().drop(columns=[\"problem\", \"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "        \n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "\n",
    "RQ21_summary_table_qubits = sum_table\n",
    "RQ21_summary_table_qubits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140abed-35a0-4fda-bc57-7dc830698824",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RQ 1.2 & 2.2 (Practical gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836c513-e040-4981-b7e9-3982e1b191f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.read_pickle(OED_file_path)\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1a64b-f7f3-4602-99ea-c630e4fb7bdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ 1.1 - Repair (practical)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6812244-df1b-4de1-ac3b-93f534d83640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repair_df = all_vals_df[all_vals_df.repair].reset_index()      \n",
    "repair_df[\"Categorisation_OED\"] = repair_df.apply(get_operator_categorisation_OED, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e939e-ee26-4fc9-8d55-e87acb90ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(repair_df.groupby(\"Categorisation_OED\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600f285-65a6-4138-8f1b-ee4944493027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for opt in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[opt])\n",
    "    rows = []\n",
    "    for problem in repair_df.problem.unique():\n",
    "        qubits=repair_df[repair_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = repair_df[repair_df.problem == problem].arbitrary.iloc[0]\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=\"Arbitrary\" if arbitrary else \"Specific\")\n",
    "        \n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(repair_df[\n",
    "                (repair_df.option == opt) & \n",
    "                (repair_df.problem == problem) & \n",
    "                (repair_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[opt] = tab\n",
    "\n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "sum_table[\"problem\"] = sum_table.problem.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "sum_table[\"Qubits\"] = sum_table.qubits.apply(lambda v: v.split(\"/\")[0].strip() + \" qubits\")\n",
    "sum_table[\"Input State\"] = sum_table.Arbitrary.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "\n",
    "sum_table[\"Problem\"] =  sum_table[\"problem\"] + \" (\" + sum_table[\"Qubits\"]+\")\"\n",
    "sum_table = sum_table.sort_values(by=[\"Arbitrary\", \"qubits\"], ascending=[False,True], key=lambda col: col.str.lower())\n",
    "sum_table.index = pd.MultiIndex.from_frame(sum_table[[\"Input State\", \"Problem\"]])\n",
    "sum_table = sum_table.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"Problem\", \"Qubits\", \"Input State\"])\n",
    "\n",
    "RQ12_table = sum_table\n",
    "RQ12_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966f1e3-a98b-43bc-9b07-8778a7e03e15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ2.1 - Optimization (practical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d82bd-33fb-4a03-831a-218b03adf35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df[all_vals_df.repair == False].reset_index()\n",
    "optimization_df[\"Categorisation_OED\"] = optimization_df.apply(get_operator_categorisation_OED, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0474a1-1af4-4e2d-83a6-785b12eb48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arbitrary)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "\n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "sum_table[\"problem\"] = sum_table.problem.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "sum_table[\"Qubits\"] = sum_table.qubits.apply(lambda v: v.split(\"/\")[0].strip() + \" qubits\")\n",
    "sum_table[\"Arbitrary\"] = sum_table.Arbitrary.apply(lambda v: v.split(\"/\")[0].strip())\n",
    "\n",
    "sum_table[\"Problem\"] = \"[\" + sum_table[\"Arbitrary\"] + \"]\" + sum_table[\"problem\"]\n",
    "sum_table = sum_table.sort_values(by=[\"Qubits\", \"problem\"], key=lambda col: col.str.lower())\n",
    "sum_table.index = pd.MultiIndex.from_frame(sum_table[[\"Qubits\", \"Problem\"]])\n",
    "sum_table = sum_table.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"Problem\", \"Qubits\"])\n",
    "\n",
    "RQ22_table = sum_table\n",
    "RQ22_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030c7bc-887c-4fcd-8d7b-f8c49ec411fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Summary Table (because the full table is too long!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb3243-45a3-4bf4-863c-9c2cb61f00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    tables[option] = pd.DataFrame(tab)\n",
    "        \n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "\n",
    "RQ22_summary_table_sum = sum_table.T.drop(columns=[\"qubits\", \"problem\", \"Arbitrary\"])\n",
    "RQ22_summary_table_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514ff6e-1eb5-4d85-af87-62a749a7c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)  \n",
    "    tab = tab.groupby(\"Arbitrary\").sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    tables[option] = tab\n",
    "\n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "\n",
    "RQ22_summary_table_input_state = sum_table\n",
    "RQ22_summary_table_input_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42185e7c-a8c2-4027-944f-a819e73c2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    tab = tab.groupby(\"qubits\").sum().drop(columns=[\"problem\", \"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "sum_table = tables[HYBRID].applymap(lambda v: f\"{v}\").add(tables[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(tables[FIXED].applymap(lambda v: f\" / {v}\"))\n",
    "\n",
    "RQ22_summary_table_qubits = sum_table\n",
    "RQ22_summary_table_qubits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca4497-3e57-4f9c-8a48-bac85536f4e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Merge Tables for RQ1 and RQ2 and produce tex output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d842bf9-f12c-4150-9a91-e54e68725e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tex_adapt_table(table, col_fix=True, space_slash=True):\n",
    "    tex = table.to_latex()\n",
    "    \n",
    "    tex = tex.replace(\"%\", \"\")\n",
    "    \n",
    "    # centre columns\n",
    "    if re.search(r\"\\{l+\\}\", tex):\n",
    "        orig_cols = re.search(r\"\\{l+\\}\", tex).group(0)\n",
    "        cols = orig_cols.replace(\"l\", \"c\")\n",
    "        tex = tex.replace(orig_cols, cols)\n",
    "    \n",
    "    if col_fix:\n",
    "        tex = tex.replace(\"multicolumn{3}{r}{\", \"multicolumn{3}{c}{\")\n",
    "        tex = tex.replace(\"multicolumn{4}{r}{\", \"multicolumn{4}{c}{\")\n",
    "        tex = tex.replace(\"multicolumn{7}{r}{\", \"multicolumn{7}{c}{\")\n",
    "        if \"Input State & Problem &  &  &  &  &  &  &  &  \\\\\\\\\" in tex:\n",
    "            tex = tex.replace(\"Input State & Problem &  &  &  &  &  &  &  &  \\\\\\\\\", \"\")        \n",
    "            tex = tex.replace(\" &  & Optimized & Pareto Equal & Worse & Faulty & Optimized & Pareto Equal & Worse & Faulty\",\n",
    "                              \"Input State & Problem & Optimized & Pareto Equal & Worse & Faulty & Optimized & Pareto Equal & Worse & Faulty\")\n",
    "        if \"qubits & Problem &  &  &  &  &  &  &  &  \\\\\\\\\" in tex:\n",
    "            tex = tex.replace(\"qubits & Problem &  &  &  &  &  &  &  &  \\\\\\\\\", \"\")\n",
    "            tex = tex.replace(\" &  & Optimized & Pareto Equal & Worse & Faulty & Optimized & Pareto Equal & Worse & Faulty\",\n",
    "                              \"Qubits & Problem & Optimized & Pareto Equal & Worse & Faulty & Optimized & Pareto Equal & Worse & Faulty\")\n",
    "        \n",
    "        if \"c\"*15 in tex:\n",
    "            tex = tex.replace(\"c\"*15, \"c|ccccccc|ccccccc\")\n",
    "        elif \"lrrrrrrrrrrrrrrr\" in tex:\n",
    "            tex = tex.replace(\"lrrrrrrrrrrrrrrr\", \"c|rrrrr|rrrrr|rrrrr\")\n",
    "        elif \"c\"*10 in tex:\n",
    "            tex = tex.replace(\"c\"*10, \"cc|cccc|cccc\")\n",
    "        elif \"c\"*9 in tex:\n",
    "            tex = tex.replace(\"c\"*9, \"c|cccc|cccc\")\n",
    "        elif \"c\"*8 in tex:\n",
    "            tex = tex.replace(\"c\"*8, \"cc|ccc|ccc\")\n",
    "        elif \"c\"*7 in tex:\n",
    "            tex = tex.replace(\"c\"*7, \"c|ccc|ccc\")\n",
    "        \n",
    "            \n",
    "    # vertically center multirow label\n",
    "    tex = tex.replace(\"\\\\multirow[t]{3}{*}\", \"\\\\multirow[t]{3}{*}[-1em]\")\n",
    "    tex = tex.replace(\"PI\", \"PI Comparison\")\n",
    "    \n",
    "    # centre column group headlines\n",
    "    tex = tex.replace(\"{l}\", \"{c}\")\n",
    "    \n",
    "    tex = tex.replace(f\"Fixed\", \"\\\\fix\")\n",
    "    tex = tex.replace(f\"Non-Hybrid\", \"\\\\non\")\n",
    "    tex = tex.replace(f\"Hybrid\", \"\\\\hyb\")\n",
    "    \n",
    "    tex = tex.replace(\"\\\\hyb_{NGen=50}\", \"NGen=50\")\n",
    "    tex = tex.replace(\"\\\\hyb_{NGen=100}\", \"NGen=100\")\n",
    "    tex = tex.replace(\"\\\\hyb_{N=100}\", \"N=100\")\n",
    "    tex = tex.replace(\"\\\\hyb_{N=200}\", \"N=200\")\n",
    "    tex = tex.replace(\"\\\\hyb_{Init=20}\", \"Init=20\")\n",
    "    tex = tex.replace(\"\\\\hyb_{Q2}\", \"\\\\texttt{Q2}\")\n",
    "\n",
    "    \n",
    "    tex = tex.replace(\"\\\\cline{1-5}\\n\\\\bottomrule\", \"\\\\bottomrule\")\n",
    "    tex = tex.replace(\"\\\\cline{1-10}\\n\\\\bottomrule\", \"\\\\bottomrule\")\n",
    "    tex = tex.replace(\"\\\\cline{1-9}\\n\\\\bottomrule\", \"\\\\bottomrule\")    \n",
    "    \n",
    "    # problem names\n",
    "    tex = tex.replace('hamiltonian_simulation_', 'hamiltonian\\_').replace('iswap_n2','iswap\\_n2')\n",
    "    tex = tex.replace('quantum_walk', 'quantum\\_walk')\n",
    "    tex = tex.replace('fredkin_n3', 'fredkin\\_n3')\n",
    "    tex = tex.replace('linearsolver_n3',  'linearsolver\\_n3')\n",
    "    tex = tex.replace('quantum_mc_F', 'quantum\\_mc\\_F')\n",
    "    tex = tex.replace('teleportation_n3',  'teleportation\\_n3')\n",
    "    tex = tex.replace('tofolli_n3', 'tofolli\\_n3')\n",
    "    tex = tex.replace('wstate_n3',  'wstate\\_n3')\n",
    "    tex = tex.replace('adder_n4',  'adder\\_n4')\n",
    "    tex = tex.replace('bell_n4', 'bell\\_n4')\n",
    "    tex = tex.replace('cat_state_n4', 'cat\\_state\\_n4')\n",
    "    tex = tex.replace('hs4_n4',  'hs4\\_n4')\n",
    "    tex = tex.replace('qrng_n4', 'qrng\\_n4')\n",
    "    tex = tex.replace('lpn_n5',  'lpn\\_n5')\n",
    "    tex = tex.replace('qec_en_n5', 'qec\\_en\\_n5')\n",
    "    \n",
    "    \n",
    "    tex = tex.replace('QSE_', 'QSE\\_')\n",
    "    tex = tex.replace('QSE2_', 'QSE2\\_')\n",
    "    tex = tex.replace('QSO_', 'QSO\\_')\n",
    "    tex = tex.replace('QG_', 'QG\\_')\n",
    "    \n",
    "    # replace rowcolor\n",
    "    \n",
    "    colored_lines = []\n",
    "    for line in tex.split(\"\\\\\\\\\"):\n",
    "        # print(\"before\", line)\n",
    "        if '[True]' in line:  # make line arbitrary\n",
    "            line = line.replace('&', '& \\\\cellcolor{specificrow}')\n",
    "            line = line.replace(\"[True]\", \"\")\n",
    "        elif '[False]' in line:  # make line specific\n",
    "            line = line.replace('&', '& \\\\cellcolor{arbitraryrow}')\n",
    "            line = line.replace(\"[False]\", \"\")\n",
    "        else:\n",
    "            line = line    \n",
    "        colored_lines.append(line)\n",
    "    \n",
    "    tex = \"\\\\\\\\\".join(colored_lines)\n",
    "    \n",
    "    # RQ1 Table:\n",
    "    # add lines between new Qubit sizes\n",
    "    tex = tex.replace(\"cline{1-10}\", \"hline\")\n",
    "    \n",
    "    # RQ2 Summary Table:\n",
    "    tex = tex.replace(\"Repair &\", \"\\\\hline Repair &\")\n",
    "    tex = tex.replace(\"Specific &\", \"\\\\hline Specific &\")\n",
    "    tex = tex.replace(\"2 qubits &\", \"\\\\hline 2 qubits &\")\n",
    "    \n",
    "    # Don't make spaces so large\n",
    "    if space_slash:\n",
    "        tex = tex.replace(\" / \", \"{\\,}/{\\,}\")\n",
    "    else:\n",
    "        tex = tex.replace(\" / \", \"/\")\n",
    "    \n",
    "    return tex\n",
    "\n",
    "def write_table_to_file(table, filepath, col_fix=True, space_slash=True):\n",
    "    tex = tex_adapt_table(table, col_fix=col_fix, space_slash=space_slash)\n",
    "    with open(filepath, 'w') as texfile:\n",
    "        texfile.write(tex)\n",
    "    return tex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc23add-2180-4112-98a2-99f70785b023",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacdee7-a7dc-40b0-bc8b-21923cc73195",
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ11_table_out = RQ11_table.copy()\n",
    "RQ12_table_out = RQ12_table.copy()\n",
    "\n",
    "RQ11_table_out.columns = pd.MultiIndex.from_product([[\"RQ1.1 (Perfect Accuracy)\"], RQ11_table_out.columns])\n",
    "RQ12_table_out.columns = pd.MultiIndex.from_product([[\"RQ1.2 (Acceptable Accuracy)\"], RQ12_table_out.columns])\n",
    "\n",
    "RQ1_table_out = pd.concat([RQ11_table_out, RQ12_table_out], axis=1)\n",
    "display(RQ1_table_out)\n",
    "\n",
    "write_table_to_file(RQ1_table_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ1.tex\", col_fix=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78840f-7f75-4a56-8960-d12b250294cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ2 -- Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab11214-66a8-45ce-8b6e-61789abe330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ21_summary_table_sum_out = RQ21_summary_table_sum.copy()\n",
    "RQ21_summary_table_input_state_out = RQ21_summary_table_input_state.copy()\n",
    "RQ21_summary_table_qubits_out = RQ21_summary_table_qubits.copy()\n",
    "\n",
    "RQ22_summary_table_sum_out = RQ22_summary_table_sum.copy()\n",
    "RQ22_summary_table_input_state_out = RQ22_summary_table_input_state.copy()\n",
    "RQ22_summary_table_qubits_out = RQ22_summary_table_qubits.copy()\n",
    "\n",
    "RQ21_summary_concat = pd.concat([RQ21_summary_table_sum_out, RQ21_summary_table_input_state_out, RQ21_summary_table_qubits_out])\n",
    "RQ21_summary_concat.columns = pd.MultiIndex.from_product([[\"RQ2.1 (Perfect Accuracy)\"], RQ21_summary_concat.columns])\n",
    "\n",
    "RQ22_summary_concat = pd.concat([RQ22_summary_table_sum_out, RQ22_summary_table_input_state_out, RQ22_summary_table_qubits_out])\n",
    "RQ22_summary_concat.columns = pd.MultiIndex.from_product([[\"RQ2.2 (Acceptable Accuracy)\"], RQ22_summary_concat.columns])\n",
    "\n",
    "RQ2_summary_out = pd.concat([RQ21_summary_concat, RQ22_summary_concat], axis=1)\n",
    "RQ2_summary_out.index = [\"Total\", \"Arbitrary\", \"Specific\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"]\n",
    "RQ2_summary_out = RQ2_summary_out.reindex([\"Total\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"])\n",
    "\n",
    "display(RQ2_summary_out)\n",
    "\n",
    "write_table_to_file(RQ2_summary_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ2-summary.tex\", col_fix=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0039-56f0-42a7-be09-03e78367f641",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ2 -- Detail Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74d15e-df70-400b-aaaa-f322c3ee9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ21_table_out = RQ21_table.copy()\n",
    "RQ22_table_out = RQ22_table.copy()\n",
    "RQ21_table_out.columns = pd.MultiIndex.from_product([[\"RQ2.1 (Perfect Accuracy)\"], RQ21_table_out.columns])\n",
    "RQ22_table_out.columns = pd.MultiIndex.from_product([[\"RQ2.2 (Acceptable Accuracy)\"], RQ22_table_out.columns])\n",
    "\n",
    "RQ2_table_out = pd.concat([RQ21_table_out, RQ22_table_out], axis=1)\n",
    "display(RQ2_table_out)\n",
    "\n",
    "write_table_to_file(RQ2_table_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ2.tex\", col_fix=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a03a03-362b-4888-9581-f95202ea9bdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RQ2 Calculate Relative Improvement (for those that optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6c170-204b-419e-b1d1-85bcef105fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_opt_cols = [\"Gates\", \"Depth\", \"NonLocalGates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90ebf6-4052-403b-9860-a423608f19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relative_optimization(row): \n",
    "    ref_num_gates, ref_depth, ref_non_local = reference_fitness_values[row.problem]\n",
    "        \n",
    "    rel_opt_num_gates = (ref_num_gates - row.num_gates) / ref_num_gates\n",
    "    rel_depth = (ref_depth - row.depth) / ref_depth\n",
    "    rel_non_local = (ref_non_local - row.num_nonloc_gates) / ref_non_local\n",
    "    \n",
    "    return rel_opt_num_gates, rel_depth, rel_non_local\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7e4c5-8f8c-4156-9f19-f925fdf4560a",
   "metadata": {},
   "source": [
    "### OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c230f927-c65e-4518-ac2d-1dfe67ecf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.read_pickle(OL_file_path)\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34415f-855a-4ef0-b39a-13d7fce18ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df[all_vals_df.repair == False].copy()\n",
    "optimization_df[\"Categorisation_OL\"] = optimization_df.apply(get_operator_categorisation_OL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222c975-1bb6-498a-ba6f-ceca2c5d7e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_df = optimization_df[optimization_df.Categorisation_OL == \"Optimized\"].copy()\n",
    "optimized_df[rel_opt_cols] = optimized_df.apply(extract_relative_optimization, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54b599-0f3d-438a-bbb8-30a581966673",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_df[(optimized_df.option == \"\") & (optimized_df.problem == \"AA4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14d2fd-3f85-4409-aefd-e57e7a2853a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_opt_problem = optimized_df[optimized_df.option.isin([HYBRID, NONHYBRID, FIXED])].groupby([\"option\", \"problem\"])[rel_opt_cols].mean().reset_index()\n",
    "\n",
    "# some of the use cases have no optimization. Therefore, the average is zero!\n",
    "avg_optimization = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    missing = 38 - len(avg_opt_problem[avg_opt_problem.option == option])\n",
    "    avg_optimization[better_options[option]] = np.concatenate([avg_opt_problem[avg_opt_problem.option == option][rel_opt_cols].to_numpy(),  np.zeros((missing, 3))]).mean(axis=0)\n",
    "\n",
    "# Q2 Optimization\n",
    "rows = []\n",
    "for name, opt_vals in QISKIT_opt_results.items():\n",
    "    if ((np.array(reference_fitness_values[name]) - np.array(opt_vals)) > 0 ).any():  # if any is better, calculate improvement\n",
    "        rows.append(extract_relative_optimization(pd.Series(dict(problem=name, num_gates=opt_vals[0], depth=opt_vals[1], num_nonloc_gates=opt_vals[2]))))\n",
    "    else:  # otherwise, use zeros\n",
    "        rows.append([0,0,0])\n",
    "        \n",
    "# add default optimization of Q2 (Qiskit)\n",
    "avg_optimization[\"Q2\"] = np.array(rows).mean(axis=0)\n",
    "    \n",
    "print(\"OL (Theoretical) in percent\")\n",
    "average_rel_improve_OL = (pd.DataFrame(avg_optimization, index=rel_opt_cols).T * 100).round(1).astype(str) + \"%\"\n",
    "average_rel_improve_OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46090325-9336-4d67-b657-8dec2b19e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677eed7-55c5-4a08-ba96-564ecc01197c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "by_approach = {}\n",
    "for opt in [HYBRID, NONHYBRID, FIXED]:\n",
    "    approach_optimized_df = optimized_df[optimized_df.option == opt]\n",
    "    grouped = (approach_optimized_df.groupby([\"problem\"])[rel_opt_cols].mean() * 100).round(1).astype(str) + \"%\"\n",
    "    # display(grouped)\n",
    "    \n",
    "    # add those rows that had now optimization...\n",
    "    add_indices = pd.Index(optimization_df.problem.unique()).difference(grouped.index)\n",
    "    add_df = pd.DataFrame(index=add_indices, columns=grouped.columns).fillna(\"---\")\n",
    "    \n",
    "    merged_df = (pd.concat([grouped,add_df]).reindex(optimization_df.problem.sort_values().unique()))\n",
    "    # merged_df.columns =  pd.MultiIndex.from_product([[better_options[opt]], merged_df.columns])\n",
    "    by_approach[opt] = merged_df\n",
    "\n",
    "# Q2 Optimization\n",
    "rows = []\n",
    "for name, opt_vals in QISKIT_opt_results.items():\n",
    "    # if name not in list(optimization_df.problem.unique()):\n",
    "    #     continue\n",
    "    if ((np.array(reference_fitness_values[name]) - np.array(opt_vals)) == 0 ).all():  # not optimized\n",
    "        rows.append([name] + [\"---\", \"---\", \"---\"])\n",
    "    elif ((np.array(reference_fitness_values[name]) - np.array(opt_vals)) > 0 ).any():  # if any is better, calculate improvement\n",
    "        extracts = extract_relative_optimization(pd.Series(dict(problem=name, num_gates=opt_vals[0], depth=opt_vals[1], num_nonloc_gates=opt_vals[2])))\n",
    "        extracts = ( (pd.Series(extracts) * 100).round(1).astype(str) + \"%\").tolist() \n",
    "        rows.append([name] + list(extracts))\n",
    "    else:  # otherwise, not optimized\n",
    "        rows.append([name] + [\"---\", \"---\", \"---\"])\n",
    "        \n",
    "# add default optimization of Q2 (Qiskit)\n",
    "by_approach[\"Q2\"] = pd.DataFrame(rows, columns=[\"problem\"] + rel_opt_cols).set_index(\"problem\")\n",
    "    \n",
    "individual_rel_improve_OL = by_approach[HYBRID].applymap(lambda v: f\"{v}\").add(by_approach[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(by_approach[FIXED].applymap(lambda v: f\" / {v}\")).add(by_approach[\"Q2\"].applymap(lambda v: f\" / {v}\"))\n",
    "individual_rel_improve_OL[\"qubits\"] = list(pd.Series(individual_rel_improve_OL.index).apply(lambda i: QUBITS_and_ARBITRARY[i][0]))\n",
    "individual_rel_improve_OL[\"arbitrary\"] = list(pd.Series(individual_rel_improve_OL.index).apply(lambda i: QUBITS_and_ARBITRARY[i][1]))\n",
    "individual_rel_improve_OL[\"problem\"] = list(pd.Series(individual_rel_improve_OL.index))\n",
    "individual_rel_improve_OL[\"Problem\"] = individual_rel_improve_OL.apply(lambda row: \"[\" + str(row[\"arbitrary\"]) + \"]\" + row.problem, axis=1)\n",
    "individual_rel_improve_OL = individual_rel_improve_OL.sort_values([\"qubits\", \"problem\"])\n",
    "individual_rel_improve_OL.index = pd.MultiIndex.from_frame(individual_rel_improve_OL[[\"qubits\", \"Problem\"]])\n",
    "individual_rel_improve_OL = individual_rel_improve_OL.drop(columns=[\"qubits\", \"problem\", \"Problem\", \"arbitrary\"])\n",
    "individual_rel_improve_OL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd713f-8193-482f-8d86-bf8c239128bf",
   "metadata": {},
   "source": [
    "### OED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aac4b2-57e5-4e1e-8c1c-20d80caf8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.read_pickle(OED_file_path)\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e132c-c35b-4076-b1fe-17d5d7bf85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df[all_vals_df.repair == False].copy()\n",
    "optimization_df[\"Categorisation_OED\"] = optimization_df.apply(get_operator_categorisation_OED, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41cb29-5cd1-47f0-ba04-9ad833a60a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_df = optimization_df[optimization_df.Categorisation_OED == \"Optimized\"].copy()\n",
    "optimized_df[rel_opt_cols] = optimized_df.apply(extract_relative_optimization, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31aaa983-5153-459e-b73f-544febbaf8cc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "optimized_df[(optimized_df.option == \"\") & (optimized_df.problem == \"AA4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58e4d4-4afa-4fe2-85f8-e560341884e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_opt_problem = optimized_df[optimized_df.option.isin([HYBRID, NONHYBRID, FIXED])].groupby([\"option\", \"problem\"])[rel_opt_cols].mean().reset_index()\n",
    "\n",
    "avg_optimization = {}\n",
    "for option in [HYBRID, NONHYBRID, FIXED]:\n",
    "    missing = 38 - len(avg_opt_problem[avg_opt_problem.option == option])\n",
    "    avg_optimization[better_options[option]] = np.concatenate([avg_opt_problem[avg_opt_problem.option == option][rel_opt_cols].to_numpy(),  np.zeros((missing, 3))]).mean(axis=0)\n",
    "\n",
    "# Q2 Optimization\n",
    "rows = []\n",
    "for name, opt_vals in QISKIT_opt_results.items():\n",
    "    if ((np.array(reference_fitness_values[name]) - np.array(opt_vals)) > 0 ).any():  # if any is better, calculate improvement\n",
    "        rows.append(extract_relative_optimization(pd.Series(dict(problem=name, num_gates=opt_vals[0], depth=opt_vals[1], num_nonloc_gates=opt_vals[2]))))\n",
    "    else:  # otherwise, use zeros\n",
    "        rows.append([0,0,0])\n",
    "        \n",
    "avg_optimization[\"Q2\"] = np.array(rows).mean(axis=0)\n",
    "    \n",
    "print(\"OED (Practical) in percent\")\n",
    "(pd.DataFrame(avg_optimization, index=rel_opt_cols).T * 100).round(2)\n",
    "\n",
    "average_rel_improve_OED = (pd.DataFrame(avg_optimization, index=rel_opt_cols).T * 100).round(1).astype(str) + \"%\"\n",
    "average_rel_improve_OED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2ecfb-3d67-4a08-9132-51228b37d697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "by_approach = {}\n",
    "for opt in [HYBRID, NONHYBRID, FIXED]:\n",
    "    approach_optimized_df = optimized_df[optimized_df.option == opt]\n",
    "    grouped = (approach_optimized_df.groupby([\"problem\"])[rel_opt_cols].mean() * 100).round(1).astype(str) + \"%\"\n",
    "    # display(grouped)\n",
    "    \n",
    "    # add those rows that had now optimization...\n",
    "    add_indices = pd.Index(optimization_df.problem.unique()).difference(grouped.index)\n",
    "    add_df = pd.DataFrame(index=add_indices, columns=grouped.columns).fillna(\"---\")\n",
    "    \n",
    "    merged_df = (pd.concat([grouped,add_df]).reindex(optimization_df.problem.sort_values().unique()))\n",
    "    # merged_df.columns =  pd.MultiIndex.from_product([[better_options[opt]], merged_df.columns])\n",
    "    by_approach[opt] = merged_df\n",
    "    # display(merged_df)\n",
    "\n",
    "# Q2 Optimization\n",
    "rows = []\n",
    "for name, opt_vals in QISKIT_opt_results.items():\n",
    "    # if name not in list(optimization_df.problem.unique()):\n",
    "    #     continue\n",
    "    if ((np.array(reference_fitness_values[name]) - np.array(opt_vals)) == 0 ).all():  # not optimized\n",
    "        rows.append([name] + [\"---\", \"---\", \"---\"])\n",
    "    elif ((np.array(reference_fitness_values[name]) - np.array(opt_vals)) > 0 ).any():  # if any is better, calculate improvement\n",
    "        extracts = extract_relative_optimization(pd.Series(dict(problem=name, num_gates=opt_vals[0], depth=opt_vals[1], num_nonloc_gates=opt_vals[2])))\n",
    "        extracts = ( (pd.Series(extracts) * 100).round(1).astype(str) + \"%\").tolist() \n",
    "        rows.append([name] + list(extracts))\n",
    "    else:  # otherwise, not optimized\n",
    "        rows.append([name] + [\"---\", \"---\", \"---\"])\n",
    "        \n",
    "# add default optimization of Q2 (Qiskit)\n",
    "by_approach[\"Q2\"] = pd.DataFrame(rows, columns=[\"problem\"] + rel_opt_cols).set_index(\"problem\")\n",
    "    \n",
    "# individual_rel_improve_OED = by_approach[HYBRID].applymap(lambda v: f\"{v}\").add(by_approach[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(by_approach[FIXED].applymap(lambda v: f\" / {v}\")).add(by_approach[\"Q2\"].applymap(lambda v: f\" / {v}\"))\n",
    "# individual_rel_improve_OED\n",
    "\n",
    "individual_rel_improve_OED = by_approach[HYBRID].applymap(lambda v: f\"{v}\").add(by_approach[NONHYBRID].applymap(lambda v: f\" / {v}\")).add(by_approach[FIXED].applymap(lambda v: f\" / {v}\")).add(by_approach[\"Q2\"].applymap(lambda v: f\" / {v}\"))\n",
    "individual_rel_improve_OED[\"qubits\"] = list(pd.Series(individual_rel_improve_OED.index).apply(lambda i: QUBITS_and_ARBITRARY[i][0]))\n",
    "individual_rel_improve_OED[\"arbitrary\"] = list(pd.Series(individual_rel_improve_OED.index).apply(lambda i: QUBITS_and_ARBITRARY[i][1]))\n",
    "individual_rel_improve_OED[\"problem\"] = list(pd.Series(individual_rel_improve_OED.index))\n",
    "individual_rel_improve_OED[\"Problem\"] = individual_rel_improve_OED.apply(lambda row: \"[\" + str(row[\"arbitrary\"]) + \"]\" + row.problem, axis=1)\n",
    "individual_rel_improve_OED = individual_rel_improve_OED.sort_values([\"qubits\", \"problem\"])\n",
    "individual_rel_improve_OED.index = pd.MultiIndex.from_frame(individual_rel_improve_OED[[\"qubits\", \"Problem\"]])\n",
    "individual_rel_improve_OED = individual_rel_improve_OED.drop(columns=[\"qubits\", \"problem\", \"Problem\", \"arbitrary\"])\n",
    "individual_rel_improve_OED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decee03d-5dab-4ab9-9d42-d89c48dfd8f0",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb8817-2aa7-4751-9862-cffb5bf10c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_rel_improve_OL_out = individual_rel_improve_OL.copy()\n",
    "individual_rel_improve_OED_out = individual_rel_improve_OED.copy()\n",
    "\n",
    "individual_rel_improve_OL_out.columns = pd.MultiIndex.from_product([[\"RQ2.1 (Perfect Accuracy)\"], individual_rel_improve_OL_out.columns])\n",
    "individual_rel_improve_OED_out.columns = pd.MultiIndex.from_product([[\"RQ2.2 (Acceptable Accuracy)\"], individual_rel_improve_OED_out.columns])\n",
    "\n",
    "individual_rel_improve_out = pd.concat([individual_rel_improve_OL_out, individual_rel_improve_OED_out], axis=1)\n",
    "display(individual_rel_improve_out)\n",
    "\n",
    "write_table_to_file(individual_rel_improve_out, paper_tables_dir / f\"{DATA_PREFIX}-rel-improve.tex\", col_fix=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ea58f-d367-43d5-8a02-124139b1d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rel_improve_OL_out = average_rel_improve_OL.copy()\n",
    "average_rel_improve_OED_out = average_rel_improve_OED.copy()\n",
    "\n",
    "average_rel_improve_OL_out.columns = pd.MultiIndex.from_product([[\"RQ2.1 (Perfect Accuracy)\"], average_rel_improve_OL_out.columns])\n",
    "average_rel_improve_OED_out.columns = pd.MultiIndex.from_product([[\"RQ2.2 (Acceptable Accuracy)\"], average_rel_improve_OED_out.columns])\n",
    "\n",
    "average_rel_improve_out = pd.concat([average_rel_improve_OL_out, average_rel_improve_OED_out], axis=1)\n",
    "display(average_rel_improve_out)\n",
    "\n",
    "write_table_to_file(average_rel_improve_out, paper_tables_dir / f\"{DATA_PREFIX}-avg-rel-improve.tex\", col_fix=True )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f657fed-5e1a-4012-b41c-79064d148008",
   "metadata": {},
   "source": [
    "tables = {}\n",
    "for option in [k for k in better_options.keys() if k not in [FIXED, NONHYBRID]]:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "    tab = pd.DataFrame(rows)\n",
    "    # display(tab)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    # display(tab)\n",
    "    tab[better_options[option]] = tab.apply(lambda row: \"/\".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab[\"Problem\"] = \"[\" + tab[\"Arbitrary\"] + \"]\" + tab[\"problem\"]\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])\n",
    "    tab.index = pd.MultiIndex.from_frame(tab[[\"qubits\", \"Problem\"]])\n",
    "    \n",
    "    tab = tab.drop(columns=[\"qubits\", \"problem\", \"Arbitrary\", \"Problem\"])\n",
    "    # tab.columns = pd.MultiIndex.from_product([[better_options[option]], tab.columns])\n",
    "    \n",
    "    \n",
    "    tables[option] = tab\n",
    "    \n",
    "    # display(tab)\n",
    "    \n",
    "RQ42_summary_table_sum = pd.concat(tables.values(), axis=1)\n",
    "RQ42_summary_table_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c3d83-4d5e-45e4-9eb0-64ea78e55bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RQ4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829416e2-e508-471a-a936-b321c010d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_solutions_max_gen(files, earliest_finish=None, max_gen=100):\n",
    "    earliest_finish_times = {}\n",
    "    if earliest_finish:\n",
    "        for earliest_finish_file in list(results_path.glob(\"*earliest_finish.csv\")):\n",
    "            problem = earliest_finish_file.stem.replace(\"_earliest_finish\", \"\")\n",
    "\n",
    "            earliest_finish_df = pd.read_csv(earliest_finish_file)\n",
    "            earliest_finish_time = earliest_finish_df.iloc[0][earliest_finish]\n",
    "            earliest_finish_times[problem] = earliest_finish_time\n",
    "    # print(\"Earliest finish times:\")\n",
    "    # pprint(earliest_finish_times)\n",
    "    best_OL_rows = []\n",
    "    best_OED_rows = []\n",
    "    # extract last gen value\n",
    "    for pi_file in sorted(files):\n",
    "        problem, seed, option = extract_info_from_file(pi_file)\n",
    "        qubits, arbitrary = QUBITS_and_ARBITRARY[problem]\n",
    "        \n",
    "        last_row = dict(problem=problem, option=option, seed=seed, qubits=qubits, arbitrary=arbitrary)\n",
    "        results_file_df = pd.read_csv(pi_file)\n",
    "    \n",
    "        if max_gen and max_gen > 0:\n",
    "            results_file_df = results_file_df[results_file_df.ngen < max_gen]  # filter max_gen\n",
    "            results_file_df[\"option\"] = f\"Ngen_{max_gen}\"\n",
    "        \n",
    "        if earliest_finish:\n",
    "            if problem not in earliest_finish_times:\n",
    "                print(\"WARNING!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                print(\"No earliest finish time for problem\", problem)\n",
    "            else:\n",
    "                results_file_df = results_file_df[results_file_df.timestamp <= earliest_finish_df[earliest_finish].iloc[0]]\n",
    "        \n",
    "        last_gen_df = results_file_df[results_file_df.ngen == results_file_df.ngen.max()].reset_index()\n",
    "        last_gen_df[\"no_error_prob_actual\"] = 1 - last_gen_df.apply(lambda row: get_actual_error_rate(row.num_gates, row.num_nonloc_gates), axis=1)\n",
    "        last_gen_df[\"OED\"] = last_gen_df.overlap * last_gen_df.no_error_prob_actual\n",
    "        \n",
    "        best_OL_row = dict(last_row)\n",
    "        best_OL_row.update(last_gen_df.sort_values(\"overlap\", ascending=False).iloc[0].to_dict())\n",
    "        best_OL_rows.append(best_OL_row)\n",
    "        \n",
    "        best_OED_row = dict(last_row)\n",
    "        best_OED_row.update(last_gen_df.sort_values(\"OED\", ascending=False).iloc[0].to_dict())\n",
    "        best_OED_rows.append(best_OED_row)\n",
    "\n",
    "    OL_df = pd.DataFrame(best_OL_rows).sort_values(by=[\"problem\", \"option\", \"seed\"])\n",
    "    OED_df = pd.DataFrame(best_OED_rows).sort_values(by=[\"problem\", \"option\", \"seed\"])\n",
    "    \n",
    "    return OL_df, OED_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82575071-3db2-4fc7-9cfb-c66e7ac6f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = list(raw_results_path.glob(\"*.csv\"))\n",
    "output_files = [f for f in output_files if \"logbook\" not in str(f)]\n",
    "output_files = [f for f in output_files if \"seed\" in str(f)]\n",
    "output_files = [f for f in output_files if \"PI.csv\" not in str(f)]\n",
    "output_files = [f for f in output_files if \"DCI.csv\" not in str(f)]\n",
    "output_files = [f for f in output_files if \"HVrefpoint.csv\" not in str(f)]\n",
    "output_files = [f for f in output_files if \"globalPareto.csv\" not in str(f)]\n",
    "output_files = [f for f in output_files if \"earliest_finish.csv\" not in str(f)]\n",
    "\n",
    "hybrid_only_files = output_files\n",
    "hybrid_only_files = [f for f in hybrid_only_files if \"gateset_fixed\" not in str(f)]\n",
    "hybrid_only_files = [f for f in hybrid_only_files if \"Q2\" not in str(f)]\n",
    "hybrid_only_files = [f for f in hybrid_only_files if \"init_pop\" not in str(f)]\n",
    "hybrid_only_files = [f for f in hybrid_only_files if \"N_100\" not in str(f)]\n",
    "hybrid_only_files = [f for f in hybrid_only_files if \"N_200\" not in str(f)]\n",
    "len(hybrid_only_files)\n",
    "# hybrid_only_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa794e-bd96-431e-9444-692d871cb8ed",
   "metadata": {},
   "source": [
    "## Get the data for the Hybrid Search after 50 and 100 generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a12078-76dc-45b1-b9de-d87bf22cc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OL_file_path_100gen = Path(f\"{DATA_PREFIX.replace('%', '')}_vals_OL_100gen.pkl\")\n",
    "OL_file_path_50gen = Path(f\"{DATA_PREFIX.replace('%', '')}_vals_OL_50gen.pkl\")\n",
    "OED_file_path_100gen = Path(f\"{DATA_PREFIX.replace('%', '')}_vals_OED_100gen.pkl\")\n",
    "OED_file_path_50gen = Path(f\"{DATA_PREFIX.replace('%', '')}_vals_OED_50gen.pkl\")\n",
    "\n",
    "ngen_100_OL_df = None\n",
    "ngen_100_OED_df = None\n",
    "\n",
    "if OL_file_path_100gen.exists() and OED_file_path_100gen.exists():\n",
    "    ngen_100_OL_df = pd.read_pickle(OL_file_path_100gen)\n",
    "    ngen_100_OED_df = pd.read_pickle(OED_file_path_100gen)\n",
    "else:\n",
    "    ngen_100_OL_df, ngen_100_OED_df = extract_best_solutions_max_gen(hybrid_only_files, max_gen=100)\n",
    "    ngen_100_OL_df.to_pickle(OL_file_path_100gen)\n",
    "    ngen_100_OED_df.to_pickle(OED_file_path_100gen)\n",
    "    \n",
    "ngen_50_OL_df = None\n",
    "ngen_50_OED_df = None\n",
    "if OL_file_path_50gen.exists() and OED_file_path_50gen.exists():\n",
    "    ngen_50_OL_df = pd.read_pickle(OL_file_path_50gen)\n",
    "    ngen_50_OED_df = pd.read_pickle(OED_file_path_50gen)\n",
    "else:\n",
    "    ngen_50_OL_df, ngen_50_OED_df = extract_best_solutions_max_gen(hybrid_only_files, max_gen=50)\n",
    "    ngen_50_OL_df.to_pickle(OL_file_path_50gen)\n",
    "    ngen_50_OED_df.to_pickle(OED_file_path_50gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bed3cd-c9e1-4962-a57d-7300398cf6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_search_settings = [k for k in better_options.keys() if k not in [FIXED, NONHYBRID]]\n",
    "hybrid_search_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fd78f-121a-4ace-8f46-a4f0d22ca98c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All - Theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a712f9c-2df0-4e11-8b48-ab3bc5e3240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.concat([pd.read_pickle(OL_file_path), ngen_100_OL_df, ngen_50_OL_df])\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)\n",
    "\n",
    "# Hybrid init_pop and Hybrid are the same for all repair use cases. \n",
    "# all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)]\n",
    "\n",
    "hybrid_init_pop_repair = all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)].copy()\n",
    "hybrid_init_pop_repair[\"option\"] = \"init_pop_20\"\n",
    "\n",
    "all_vals_df = pd.concat([all_vals_df, hybrid_init_pop_repair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b401b-8f94-47cb-bb5f-23e5c4121d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df  #[all_vals_df.repair == False].reset_index()\n",
    "optimization_df[\"Categorisation_OL\"] = optimization_df.apply(get_operator_categorisation_OL, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200fe9b-6b0c-4624-8417-f6ab5f5efd35",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b854d9a-9d14-4abd-b249-934e25afbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab = tab.sum()\n",
    "    tab = pd.DataFrame(tab).T\n",
    "    \n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\"])    \n",
    "\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ41_summary_table_sum = pd.concat(tables.values(), axis=1)\n",
    "RQ41_summary_table_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998e373-aa8f-4c81-a800-3c5995cf1354",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Repair/Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e5ecb-a6c8-4df4-810c-bed10de3468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        is_repair = optimization_df[optimization_df.problem == problem].repair.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb, repair=(\"Repair\" if is_repair else \"Optimization\"))\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab = tab.groupby(\"repair\").sum()\n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\"])    \n",
    "\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ41_summary_table_repair = pd.concat(tables.values(), axis=1)\n",
    "RQ41_summary_table_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009a6ca-de42-40e8-901a-0f173e8f30f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Input State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4848d1f-eea0-46ce-8888-e06e79ec4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab = tab.groupby(\"Arbitrary\").sum()\n",
    "    # display(tab)\n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\"])    \n",
    "\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ41_summary_table_input_state = pd.concat(tables.values(), axis=1)\n",
    "RQ41_summary_table_input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e3209-1d6f-4aa0-86a1-75d39c96520d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f46959-28b1-4996-81c3-bc49e6b69961",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab[\"qubits\"] = tab.qubits.astype(str) + \" qubits\"\n",
    "    tab = tab.groupby(\"qubits\").sum()\n",
    "    \n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])\n",
    "    tab = tab.drop(columns=[\"problem\", \"Arbitrary\"])    \n",
    "    tables[option] = tab\n",
    "    \n",
    "    # display(tab)\n",
    "    \n",
    "RQ41_summary_table_qubits = pd.concat(tables.values(), axis=1)\n",
    "RQ41_summary_table_qubits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139fcdd-5e1c-469a-8d1e-1101920b7c91",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All - Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17b93d-b120-4328-86dd-32180c2d121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.concat([pd.read_pickle(OED_file_path), ngen_100_OED_df, ngen_50_OED_df])\n",
    "\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)\n",
    "\n",
    "hybrid_init_pop_repair = all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)].copy()\n",
    "hybrid_init_pop_repair[\"option\"] = \"init_pop_20\"\n",
    "\n",
    "all_vals_df = pd.concat([all_vals_df, hybrid_init_pop_repair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9972f-cdac-4d43-945c-549701aa0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df  #[all_vals_df.repair == False].reset_index()\n",
    "optimization_df[\"Categorisation_OED\"] = optimization_df.apply(get_operator_categorisation_OED, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07fbdf-18ec-4f00-be7d-f6c2517ca190",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85508724-c076-4a0a-93e9-9306e0c5e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab = tab.sum()\n",
    "    tab = pd.DataFrame(tab).T\n",
    "    \n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\"])    \n",
    "\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ42_summary_table_sum = pd.concat(tables.values(), axis=1)\n",
    "RQ42_summary_table_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e623e-cfc5-4e37-8266-db6910d35a06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Repair/Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4a45e-6a1a-4240-88aa-e8edcacde524",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        repair = optimization_df[optimization_df.problem == problem].repair.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb, repair=\"Repair\" if repair else \"Optimization\")\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab = tab.groupby(\"repair\").sum()\n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\"])    \n",
    "\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ42_summary_table_repair = pd.concat(tables.values(), axis=1)\n",
    "RQ42_summary_table_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23880f9d-16cc-47be-a1ff-14a31b13a338",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Input State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfab1a2-0f89-46a6-ad4f-986656a3eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab = tab.groupby(\"Arbitrary\").sum()\n",
    "    # display(tab)\n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\"])    \n",
    "\n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ42_summary_table_input_state = pd.concat(tables.values(), axis=1)\n",
    "RQ42_summary_table_input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df01326-3a40-4f33-81d8-f0be882798ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b1f27-5829-4da2-ba5a-fe13888d10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    tables[option] = tab\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    tab[\"qubits\"] = tab.qubits.astype(str) + \" qubits\"\n",
    "    tab = tab.groupby(\"qubits\").sum()\n",
    "    \n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])\n",
    "    tab = tab.drop(columns=[\"problem\", \"Arbitrary\"])    \n",
    "    tables[option] = tab\n",
    "    \n",
    "    # display(tab)\n",
    "    \n",
    "RQ42_summary_table_qubits = pd.concat(tables.values(), axis=1)\n",
    "RQ42_summary_table_qubits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8602058-4b7e-42c0-81fa-f9b0bffebbfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527bd8f-d17c-46f8-8846-a2c9dfb9430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ41_summary_table_sum_out = RQ41_summary_table_sum.copy()\n",
    "RQ41_summary_table_repair_out = RQ41_summary_table_repair.copy()\n",
    "RQ41_summary_table_input_state_out = RQ41_summary_table_input_state.copy()\n",
    "RQ41_summary_table_qubits_out = RQ41_summary_table_qubits.copy()\n",
    "\n",
    "RQ42_summary_table_sum_out = RQ42_summary_table_sum.copy()\n",
    "RQ42_summary_table_repair_out = RQ42_summary_table_repair.copy()\n",
    "RQ42_summary_table_input_state_out = RQ42_summary_table_input_state.copy()\n",
    "RQ42_summary_table_qubits_out = RQ42_summary_table_qubits.copy()\n",
    "\n",
    "RQ41_summary_concat = pd.concat([RQ41_summary_table_sum_out, RQ41_summary_table_repair_out, RQ41_summary_table_input_state_out, RQ41_summary_table_qubits_out])\n",
    "RQ41_summary_concat.columns = pd.MultiIndex.from_product([[\"RQ3.1 (Perfect Accuracy)\"], RQ41_summary_concat.columns])\n",
    "\n",
    "RQ42_summary_concat = pd.concat([RQ42_summary_table_sum_out, RQ42_summary_table_repair_out, RQ42_summary_table_input_state_out, RQ42_summary_table_qubits_out])\n",
    "RQ42_summary_concat.columns = pd.MultiIndex.from_product([[\"RQ3.2 (Acceptable Accuracy)\"], RQ42_summary_concat.columns])\n",
    "\n",
    "RQ4_summary_out = pd.concat([RQ41_summary_concat, RQ42_summary_concat], axis=1)\n",
    "RQ4_summary_out.index = [\"Total\", \"Repair\", \"Optimization\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"]\n",
    "# RQ4_summary_out = RQ4_summary_out.reindex([\"Total\", \"Repair\", \"Optimization\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"])\n",
    "display(RQ4_summary_out)\n",
    "\n",
    "\n",
    "print(\"Perfect Accuracy\")\n",
    "RQ41_summary_concat_out = pd.concat([RQ41_summary_table_sum_out, RQ41_summary_table_repair_out, RQ41_summary_table_input_state_out, RQ41_summary_table_qubits_out])\n",
    "RQ41_summary_concat_out.index = [\"Total\", \"Repair\", \"Optimization\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"]\n",
    "# RQ41_summary_concat_out = RQ41_summary_concat_out.reindex([\"Total\", \"Repair\", \"Optimization\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"])\n",
    "display(RQ41_summary_concat_out)\n",
    "\n",
    "print(\"Acceptable Accuracy\")\n",
    "RQ42_summary_concat_out = pd.concat([RQ42_summary_table_sum_out, RQ42_summary_table_repair_out, RQ42_summary_table_input_state_out, RQ42_summary_table_qubits_out])\n",
    "RQ42_summary_concat_out.index = [\"Total\", \"Repair\", \"Optimization\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"]\n",
    "# RQ42_summary_concat_out = RQ42_summary_concat_out.reindex([\"Total\", \"Repair\", \"Optimization\", \"Specific\", \"Arbitrary\", \"2 qubits\", \"3 qubits\", \"4 qubits\", \"5 qubits\"])\n",
    "display(RQ42_summary_concat_out)\n",
    "\n",
    "\n",
    "write_table_to_file(RQ41_summary_concat_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ4-summary-perfect.tex\", col_fix=False, space_slash=False)\n",
    "write_table_to_file(RQ42_summary_concat_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ4-summary-acceptable.tex\", col_fix=False, space_slash=False )\n",
    "write_table_to_file(RQ4_summary_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ4-summary.tex\", col_fix=True, space_slash=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65997e0f-4ca1-4bff-88ce-1d02a5a7aeed",
   "metadata": {},
   "source": [
    "## Individual Problems -- Theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743807bb-596e-4fb0-af69-19117d9b4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.concat([pd.read_pickle(OL_file_path), ngen_100_OL_df, ngen_50_OL_df])\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)\n",
    "\n",
    "# Hybrid init_pop and Hybrid are the same for all repair use cases. \n",
    "# all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)]\n",
    "\n",
    "hybrid_init_pop_repair = all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)].copy()\n",
    "hybrid_init_pop_repair[\"option\"] = \"init_pop_20\"\n",
    "\n",
    "all_vals_df = pd.concat([all_vals_df, hybrid_init_pop_repair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d110d-137f-408e-bc3c-830fe036b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df  #[all_vals_df.repair == False].reset_index()\n",
    "optimization_df[\"Categorisation_OL\"] = optimization_df.apply(get_operator_categorisation_OL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66e1c8-143f-460f-b3f4-5821c96ef4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OL == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    # tab = tab.sum()\n",
    "    # tab = pd.DataFrame(tab).T\n",
    "    \n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab[\"qubits\"] = tab.qubits.astype(str) + \" qubits\"\n",
    "    tab[\"arbitrary\"] = tab.problem.apply(lambda i: QUBITS_and_ARBITRARY[i][1])\n",
    "    tab[\"Problem\"] = tab.apply(lambda row: \"[\" + str(row[\"arbitrary\"]) + \"]\" + row.problem, axis=1)\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab.index = pd.MultiIndex.from_frame(tab[[\"qubits\", \"Problem\"]])\n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"arbitrary\", \"Problem\"])    \n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ41_individual_concat = pd.concat(tables.values(), axis=1)\n",
    "RQ41_individual_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ce6c7-97d7-49ad-98c4-14fa7ad89604",
   "metadata": {},
   "source": [
    "## Individual Problems -- Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f96932-b7d3-4b85-90d9-335959bc8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals_df = pd.concat([pd.read_pickle(OED_file_path), ngen_100_OL_df, ngen_50_OL_df])\n",
    "all_vals_df[\"repair\"] = all_vals_df.problem.apply(lambda p: p in repair_circuits)\n",
    "\n",
    "# Hybrid init_pop and Hybrid are the same for all repair use cases. \n",
    "# all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)]\n",
    "\n",
    "hybrid_init_pop_repair = all_vals_df[all_vals_df.repair & (all_vals_df.option == HYBRID)].copy()\n",
    "hybrid_init_pop_repair[\"option\"] = \"init_pop_20\"\n",
    "\n",
    "all_vals_df = pd.concat([all_vals_df, hybrid_init_pop_repair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208035b-21f3-4d9d-82c2-abb9f2b80e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_df = all_vals_df  #[all_vals_df.repair == False].reset_index()\n",
    "optimization_df[\"Categorisation_OED\"] = optimization_df.apply(get_operator_categorisation_OL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07611ec-dcfd-4b00-bbab-238487206eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tables = {}\n",
    "for option in hybrid_search_settings:\n",
    "    print(better_options[option])\n",
    "    rows = []\n",
    "    for problem in optimization_df.problem.unique():\n",
    "        # print(problem)\n",
    "        qubits=optimization_df[optimization_df.problem == problem].qubits.iloc[0]\n",
    "        arbitrary = optimization_df[optimization_df.problem == problem].arbitrary.iloc[0]\n",
    "        if arbitrary:\n",
    "            arb = \"Arbitrary\"\n",
    "        else:\n",
    "            arb = \"Specific\"\n",
    "        row = dict(problem = problem, qubits=qubits, Arbitrary=arb)\n",
    "\n",
    "        for which in [\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]:\n",
    "            row[which] = len(optimization_df[\n",
    "                (optimization_df.option == option) & \n",
    "                (optimization_df.problem == problem) & \n",
    "                (optimization_df.Categorisation_OED == which)                \n",
    "            ])\n",
    "        rows.append(row)\n",
    "        \n",
    "    tab = pd.DataFrame(rows)\n",
    "    # tab = tab.reindex(tab[\"Arbitrary\"])\n",
    "    tables[option] = tab\n",
    "    \n",
    "    # tab = tab.sum().drop(columns=[\"problem\", \"qubits\"])\n",
    "    # tables[option] = pd.DataFrame(tab)\n",
    "    \n",
    "for option, tab in tables.items():\n",
    "    # tab = tab.sum()\n",
    "    # tab = pd.DataFrame(tab).T\n",
    "    \n",
    "    tab[better_options[option]] = tab.apply(lambda row: \" / \".join(row[[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"]].astype(str)), axis=1)\n",
    "    tab = tab.drop(columns=[\"Optimized\", \"Pareto Equal\", \"Worse\", \"Faulty\"])\n",
    "    tab[\"qubits\"] = tab.qubits.astype(str) + \" qubits\"\n",
    "    tab[\"arbitrary\"] = tab.problem.apply(lambda i: QUBITS_and_ARBITRARY[i][1])\n",
    "    tab[\"Problem\"] = tab.apply(lambda row: \"[\" + str(row[\"arbitrary\"]) + \"]\" + row.problem, axis=1)\n",
    "    tab = tab.sort_values([\"qubits\", \"problem\"])  \n",
    "    tab.index = pd.MultiIndex.from_frame(tab[[\"qubits\", \"Problem\"]])\n",
    "    tab = tab.drop(columns=[\"problem\", \"qubits\", \"Arbitrary\", \"arbitrary\", \"Problem\"])    \n",
    "    tables[option] = tab\n",
    "    # display(tab)\n",
    "    \n",
    "RQ42_individual_concat = pd.concat(tables.values(), axis=1)\n",
    "RQ42_individual_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f1aeb-8abb-49db-a3a1-81a3dbe063b5",
   "metadata": {},
   "source": [
    "## Merge Individual Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143ac36-2206-443f-8269-7fa5b803d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ41_individual_concat_out = RQ41_individual_concat.copy()\n",
    "RQ42_individual_concat_out = RQ42_individual_concat.copy()\n",
    "\n",
    "display(RQ41_individual_concat_out)\n",
    "display(RQ42_individual_concat_out)\n",
    "\n",
    "RQ41_individual_concat_out_multi_index = RQ41_individual_concat_out.copy()\n",
    "RQ42_individual_concat_out_multi_index = RQ42_individual_concat_out.copy()\n",
    "RQ41_individual_concat_out_multi_index.columns = pd.MultiIndex.from_product([[\"RQ4.1 (Perfect Accuracy)\"], RQ41_individual_concat_out.columns])\n",
    "RQ42_individual_concat_out_multi_index.columns = pd.MultiIndex.from_product([[\"RQ2.2 (Acceptable Accuracy)\"], RQ42_individual_concat_out.columns])\n",
    "RQ4_individual_concat_out = pd.concat([RQ41_individual_concat_out_multi_index, RQ42_individual_concat_out_multi_index], axis=1)\n",
    "display(RQ4_individual_concat_out)\n",
    "\n",
    "# write_table_to_file(individual_rel_improve_out, paper_tables_dir / f\"{DATA_PREFIX}-rel-improve.tex\", col_fix=True )\n",
    "write_table_to_file(RQ41_individual_concat_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ4-individual-perfect.tex\", col_fix=False, space_slash=False)\n",
    "write_table_to_file(RQ42_individual_concat_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ4-individual-acceptable.tex\", col_fix=False, space_slash=False )\n",
    "write_table_to_file(RQ4_summary_out, paper_tables_dir / f\"{DATA_PREFIX}-RQ4-individual.tex\", col_fix=True, space_slash=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
